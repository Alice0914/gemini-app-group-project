{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNHH2EMXRyI2Rzuz9FPN9Gh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hwth4U3znU_R","executionInfo":{"status":"ok","timestamp":1720990795128,"user_tz":420,"elapsed":20451,"user":{"displayName":"Nick Lee","userId":"01521670819627041880"}},"outputId":"49d24a73-1b64-45e7-e6fb-78582ce18773"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"code","source":["%pip install -U --quiet langchain-google-genai langchain tiktoken pypdf sentence_transformers chromadb langchain_community PyPDF2\n","%pip install --upgrade --user google-cloud-aiplatform pymupdf rich\n","%pip install langchain-community pypdf openai datasets ray --quiet\n","%pip install accelerate -U\n","%pip install datasets seqeval tiktoken rouge_score openai --quiet\n","%pip install nltk\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9m6Ky92YnjJg","executionInfo":{"status":"ok","timestamp":1720990992274,"user_tz":420,"elapsed":197148,"user":{"displayName":"Nick Lee","userId":"01521670819627041880"}},"outputId":"d6d1acd1-199f-4183-e74e-3130aa26f747"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.6/983.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.7/295.7 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m581.4/581.4 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.2/164.2 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m366.3/366.3 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/92.2 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.9/59.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.0/107.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.7/283.7 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m221.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m718.3/718.3 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.5/130.5 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m81.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: google-cloud-aiplatform in /usr/local/lib/python3.10/dist-packages (1.59.0)\n","Collecting pymupdf\n","  Downloading PyMuPDF-1.24.7-cp310-none-manylinux2014_x86_64.whl (3.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (13.7.1)\n","Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (2.16.2)\n","Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (2.27.0)\n","Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (1.24.0)\n","Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (3.20.3)\n","Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (24.1)\n","Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (2.8.0)\n","Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (3.21.0)\n","Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (1.12.4)\n","Requirement already satisfied: shapely<3.0.0dev in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (2.0.4)\n","Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (2.8.2)\n","Requirement already satisfied: docstring-parser<1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (0.16)\n","Collecting PyMuPDFb==1.24.6 (from pymupdf)\n","  Downloading PyMuPDFb-1.24.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (15.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich) (2.16.1)\n","Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.63.2)\n","Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (2.31.0)\n","Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.64.1)\n","Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.48.2)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (5.3.3)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (0.4.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (4.9)\n","Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.3.3)\n","Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.7.1)\n","Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.8.2)\n","Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform) (0.13.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich) (0.1.2)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3->google-cloud-aiplatform) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3->google-cloud-aiplatform) (2.20.1)\n","Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3->google-cloud-aiplatform) (4.12.2)\n","Requirement already satisfied: numpy<3,>=1.14 in /usr/local/lib/python3.10/dist-packages (from shapely<3.0.0dev->google-cloud-aiplatform) (1.25.2)\n","Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.5.0)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (0.6.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0dev,>=2.7.2->google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.16.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (2024.7.4)\n","Installing collected packages: PyMuPDFb, pymupdf\n","\u001b[33m  WARNING: The script pymupdf is installed in '/root/.local/bin' which is not on PATH.\n","  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33m  WARNING: The script pymupdf is installed in '/root/.local/bin' which is not on PATH.\n","  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n","\u001b[0mSuccessfully installed PyMuPDFb-1.24.6 pymupdf-1.24.7\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.5/328.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.7/65.7 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n","google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n","ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 16.1.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mCollecting accelerate\n","  Downloading accelerate-0.32.1-py3-none-any.whl (314 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.1/314.1 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.0+cu121)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.4)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.15.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.20.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.5.82)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.7.4)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","Installing collected packages: accelerate\n","Successfully installed accelerate-0.32.1\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"]}]},{"cell_type":"code","source":["!pip3 install --quiet --upgrade google-cloud-aiplatform\n"],"metadata":{"id":"4BB0R9a4pjit","executionInfo":{"status":"ok","timestamp":1720991041758,"user_tz":420,"elapsed":8926,"user":{"displayName":"Nick Lee","userId":"01521670819627041880"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["%pip install openai==0.28"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0Fw42f0b8ugn","executionInfo":{"status":"ok","timestamp":1720991058519,"user_tz":420,"elapsed":16765,"user":{"displayName":"Nick Lee","userId":"01521670819627041880"}},"outputId":"600c1299-9db3-447a-c3e0-c6c1c189c830"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting openai==0.28\n","  Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.32.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.4)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.9.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.7.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n","Installing collected packages: openai\n","  Attempting uninstall: openai\n","    Found existing installation: openai 1.35.13\n","    Uninstalling openai-1.35.13:\n","      Successfully uninstalled openai-1.35.13\n","Successfully installed openai-0.28.0\n"]}]},{"cell_type":"code","source":["# import vertexai\n","# from vertexai.preview.language_models import TextGenerationModel\n","\n","# # Set your Google Cloud project and location\n","# project_id = ''\n","# location = ''  # e.g., 'us-central1'\n","\n","# # Initialize Vertex AI\n","# vertexai.init(project=project_id, location=location)\n","\n","# # Define the text you want to summarize\n","# text_to_summarize = \"\"\"\n","# Gemini is an innovative AI model developed by Google. It's designed for a wide range of\n","# applications, including text generation, summarization, translation, and code completion.\n","# One of Gemini's key strengths is its ability to handle both text and image inputs,\n","# making it a versatile tool for tasks like image captioning and visual question answering.\n","# \"\"\"\n","\n","# # Instantiate the Gemini model\n","# model = TextGenerationModel.from_pretrained(\"gemini-1.5-flash\")\n","\n","# # Configure the summarization parameters\n","# temperature = 0.2  # Lower for more focused summaries\n","# max_output_tokens = 100\n","\n","# # Generate the summary\n","# response = model.predict(\n","#     prompt=f\"Please summarize the following text:\\n{text_to_summarize}\",\n","#     temperature=temperature,\n","#     max_output_tokens=max_output_tokens\n","# )\n","\n","# # Extract and print the summary\n","# summary = response.text\n","# print(summary)"],"metadata":{"id":"JUMRkPB0r3jt","executionInfo":{"status":"ok","timestamp":1720985402551,"user_tz":420,"elapsed":385,"user":{"displayName":"Nick Lee","userId":"01521670819627041880"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# from IPython.display import Markdown\n","# from concurrent.futures import ThreadPoolExecutor, as_completed\n","# # from datasets import DatasetDict, Dataset\n","# from huggingface_hub import hf_hub_download\n","# from itertools import tee, islice, chain\n","# from langchain.document_loaders import PyPDFLoader\n","# from langchain.embeddings import HuggingFaceEmbeddings\n","# from langchain.prompts import ChatPromptTemplate\n","# from langchain.schema.runnable import RunnableMap\n","# from langchain.text_splitter import RecursiveCharacterTextSplitter\n","# from langchain.vectorstores import Chroma\n","# from langchain_google_genai import ChatGoogleGenerativeAI\n","# from markdown import Markdown\n","# #from sentence_transformers import SentenceTransformer\n","# from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n","# # from transformers import AutoModelForMaskedLM, TrainingArguments, Trainer\n","# from transformers import AutoTokenizer, DataCollatorForLanguageModeling\n","# from transformers import pipeline\n","# import PyPDF2\n","# import numpy as np\n","# import openai\n","# import os\n","# import pandas as pd\n","# import pypdf\n","# import ray\n","# import re\n","# import torch"],"metadata":{"id":"iaIIc8JunjO1","executionInfo":{"status":"ok","timestamp":1720934164126,"user_tz":420,"elapsed":2299,"user":{"displayName":"Nick Lee","userId":"01521670819627041880"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["import os\n","import re\n","import PyPDF2\n","import pandas as pd\n","from markdown import Markdown\n","from langchain_google_genai import ChatGoogleGenerativeAI\n","# from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain.vectorstores import Chroma\n","from langchain.document_loaders import PyPDFLoader\n","from langchain.embeddings import HuggingFaceEmbeddings\n","from langchain.prompts import ChatPromptTemplate\n","from langchain.schema.runnable import RunnableMap\n","from concurrent.futures import ThreadPoolExecutor, as_completed\n","\n","# from langchain.text_splitter import NLTKTextSplitter\n","from langchain.text_splitter import SpacyTextSplitter\n","from langchain.text_splitter import MarkdownTextSplitter\n","\n"],"metadata":{"id":"X0wtTZ1NsPZo","executionInfo":{"status":"ok","timestamp":1720991071102,"user_tz":420,"elapsed":5828,"user":{"displayName":"Nick Lee","userId":"01521670819627041880"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["os.environ[\"GOOGLE_API_KEY\"] = \"\""],"metadata":{"id":"em-C2RwEnjL8","executionInfo":{"status":"ok","timestamp":1720991071103,"user_tz":420,"elapsed":3,"user":{"displayName":"Nick Lee","userId":"01521670819627041880"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Define a Document class to store page content and metadata\n","class Document:\n","    def __init__(self, page_content, metadata):\n","        self.page_content = page_content\n","        self.metadata = metadata\n","\n","    def __repr__(self):\n","        return f\"Document(page_content={self.page_content!r}, metadata={self.metadata})\"\n","\n","# Define a DataLoader class to handle the loading and chunking of PDF data\n","class DataLoaderParentChildChunks:\n","    def __init__(self, input_file, parent_chunk_size, child_chunk_size, chunk_overlap):\n","        self.input_file = input_file\n","        self.parent_chunk_size = parent_chunk_size\n","        self.child_chunk_size = child_chunk_size\n","        self.chunk_overlap = chunk_overlap\n","\n","    # Get the total number of pages in the PDF\n","    def get_total_pages(self):\n","        with open(self.input_file, 'rb') as file:\n","            reader = PyPDF2.PdfReader(file)\n","            total_pages = len(reader.pages)\n","        return total_pages\n","\n","    # Load a specific page from the PDF\n","    def load_pdf_page(self, page_num):\n","        with open(self.input_file, 'rb') as file:\n","            reader = PyPDF2.PdfReader(file)\n","            page = reader.pages[page_num]\n","            page_text = page.extract_text()\n","        return page_text, page_num + 1\n","\n","    # Split text into sentences using regular expressions\n","    def split_into_sentences(self, text):\n","        sentence_endings = re.compile(r'(?<=[.!?])\\s+(?=[A-Z])')\n","        return sentence_endings.split(text)\n","\n","    # Create chunks from sentences with specified chunk size and overlap\n","    def create_chunks(self, sentences, page_numbers, chunk_size):\n","        chunks = []\n","        num_sentences = len(sentences)\n","        step = chunk_size - self.chunk_overlap\n","        for i in range(0, num_sentences, step):\n","            chunk_sentences = sentences[i:i + chunk_size]\n","            chunk_pages = page_numbers[i:i + chunk_size]\n","            chunk = ' '.join(chunk_sentences)\n","            if chunk:\n","                chunks.append({\n","                    'Text': chunk,\n","                    'Source': self.input_file,\n","                    'Page': ', '.join(map(str, sorted(set(chunk_pages))))\n","                })\n","        return chunks\n","\n","    # Main function to load PDF, split into sentences, and create chunks\n","    def run(self, num_pages=None):\n","        total_pages = self.get_total_pages()\n","        if num_pages is None:\n","            num_pages = total_pages\n","\n","        combined_text = \"\"\n","        page_texts = []\n","\n","        # Use ThreadPoolExecutor to load pages in parallel\n","        with ThreadPoolExecutor() as executor:\n","            future_to_page = {executor.submit(self.load_pdf_page, page_num): page_num for page_num in range(min(num_pages, total_pages))}\n","            aa = 1\n","            for future in as_completed(future_to_page):\n","                page_text, page_num = future.result()\n","                page_texts.append((page_text, page_num))\n","                combined_text += page_text + \" \"\n","\n","        # Add first and last page texts again for context\n","        if total_pages > 0:\n","            first_page_text = self.load_pdf_page(0)[0]\n","            last_page_text = self.load_pdf_page(total_pages - 1)[0]\n","            page_texts.insert(1, (first_page_text, 1))\n","            page_texts.append((last_page_text, total_pages))\n","            combined_text = first_page_text + \" \" + combined_text + \" \" + last_page_text\n","\n","        # Split combined text into sentences and track page numbers\n","        sentences = []\n","        page_numbers = []\n","        for page_text, page_num in page_texts:\n","            # Splitters\n","            #############\n","            # page_sentences = self.split_into_sentences(page_text)\n","\n","            # text_splitter = NLTKTextSplitter()\n","\n","            text_splitter = SpacyTextSplitter()\n","            page_sentences = text_splitter.split_text(page_text)\n","\n","            # splitter = MarkdownTextSplitter(chunk_size=100, chunk_overlap=0)\n","            # page_sentences = splitter.create_documents([page_text])\n","\n","            sentences.extend(page_sentences)\n","            page_numbers.extend([page_num] * len(page_sentences))\n","\n","        # Create parentchunks and child chunks\n","        parent_chunks = self.create_chunks(sentences, page_numbers, self.parent_chunk_size)\n","        child_chunks = self.create_chunks(sentences, page_numbers, self.child_chunk_size)\n","\n","        # Create document texts from the child chunks\n","        child_documents = []\n","        for idx, row in pd.DataFrame(child_chunks).iterrows():\n","            page_content = row['Text']\n","            metadata = {'source': row['Source'], 'page': row['Page']}\n","            child_documents.append(Document(page_content=page_content, metadata=metadata))\n","\n","\n","\n","\n","        # Create document texts from the parent chunks\n","        parent_documents = []\n","        for idx, row in pd.DataFrame(parent_chunks).iterrows():\n","            page_content = row['Text']\n","            metadata = {'source': row['Source'], 'page': row['Page']}\n","            parent_documents.append(Document(page_content=page_content, metadata=metadata))\n","\n","        return child_documents, parent_documents"],"metadata":{"id":"LWgtSDFcnjRY","executionInfo":{"status":"ok","timestamp":1720992173218,"user_tz":420,"elapsed":212,"user":{"displayName":"Nick Lee","userId":"01521670819627041880"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["def ask_question_with_parameters(quest):\n","    # Load and split PDF using DataLoaderParentChildChunks\n","    pdf_path = \"/content/drive/MyDrive/GenAI_Handbook.pdf\"\n","    loader = DataLoaderParentChildChunks(pdf_path, parent_chunk_size=500, child_chunk_size=200, chunk_overlap=100)\n","    child_documents, parent_documents = loader.run()\n","\n","    # Initialize HuggingFace Embeddings\n","    # model_name = \"sentence-transformers/all-MiniLM-L6-v2\" # Larger and potentially more accurate than all-mpnet-base-v2, but slower and requires more resources.\n","    model_name = \"bert-base-uncased\"\n","    hf = HuggingFaceEmbeddings(\n","        model_name=model_name,\n","        model_kwargs={'device': 'cpu'},\n","        encode_kwargs={'normalize_embeddings': True}\n","    )\n","\n","    # Create a document search index and save embeddings in vector DB\n","    child_docsearch = Chroma.from_documents(child_documents, hf)\n","    parent_docsearch = Chroma.from_documents(parent_documents, hf)\n","\n","    # Configure the retrieve\n","    retriever = child_docsearch.as_retriever(\n","        search_type=\"mmr\",\n","        search_kwargs={\"k\": 5, \"fetch_k\": 152}\n","    )\n","\n","    template1 = \"\"\"Answer the question based only on the following context:\n","    {context}\n","\n","    Question: {question}\n","    \"\"\"\n","\n","    template = \"\"\"Act like a Generative AI Customer Support Expert well-versed with Generative AI products and features.\n","    For your reference, pertinent information for generative AI products is captured below.\n","\n","    {context}\n","\n","    Thought Process:\n","    1. Read and anlyze the whole input across all lines. Identify and dwell upon the intents behind the provided input.\n","    2. Rephrase the input to make it clearer to an AI Bot. Discard irrelevant PII details and dates, times.\n","\n","    Now your turn!\n","\n","    input: {question}\n","    \"\"\"\n","\n","    prompt = ChatPromptTemplate.from_template(template)\n","\n","    gemini = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0)\n","\n","    chain = RunnableMap({\n","        \"context\": lambda x: retriever.get_relevant_documents(x['question']),\n","        \"question\": lambda x: x['question']\n","    }) | prompt | gemini\n","\n","    response = chain.invoke({'question': quest})\n","\n","    # Find the parent chunks related to the retrieved child chunks\n","    relevant_child_chunks = retriever.get_relevant_documents(quest)\n","    relevant_parent_chunks = []\n","    for child_chunk in relevant_child_chunks:\n","        parent_docs = parent_docsearch.as_retriever(\n","            search_type=\"mmr\",\n","            search_kwargs={\"k\": 5}\n","        ).get_relevant_documents(child_chunk.page_content)\n","        relevant_parent_chunks.extend(parent_docs)\n","\n","    # Ensure relevant_parent_chunks is unique\n","    unique_parent_chunks = {doc.page_content: doc for doc in relevant_parent_chunks}.values()\n","\n","    # Update the context in the template with the unique parent chunks\n","    parent_context = ' '.join([doc.page_content for doc in unique_parent_chunks])\n","    template = template.replace(\"{context}\", parent_context)\n","\n","    response = chain.invoke({'question': quest})\n","    return response.content\n","\n"],"metadata":{"id":"Q9e1A2khnjTv","executionInfo":{"status":"ok","timestamp":1720992178203,"user_tz":420,"elapsed":182,"user":{"displayName":"Nick Lee","userId":"01521670819627041880"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["quest = \"what is Token Consumption?\"\n","ask_question_with_parameters(quest)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":437},"id":"LwxsRjSinjvJ","executionInfo":{"status":"ok","timestamp":1720992293209,"user_tz":420,"elapsed":112500,"user":{"displayName":"Nick Lee","userId":"01521670819627041880"}},"outputId":"a529f091-0189-4e93-d31d-a877a5487d74"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name bert-base-uncased. Creating a new one with mean pooling.\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 152 is greater than number of elements in index 14, updating n_results = 14\n","WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 152 is greater than number of elements in index 14, updating n_results = 14\n","WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 20 is greater than number of elements in index 14, updating n_results = 14\n","WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 20 is greater than number of elements in index 14, updating n_results = 14\n","WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 20 is greater than number of elements in index 14, updating n_results = 14\n","WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 20 is greater than number of elements in index 14, updating n_results = 14\n","WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 20 is greater than number of elements in index 14, updating n_results = 14\n","WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 152 is greater than number of elements in index 14, updating n_results = 14\n","WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n","WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n","WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 8.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n","WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 16.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n","WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 32.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"]},{"output_type":"execute_result","data":{"text/plain":["\"Sure, I can help you with that.\\n\\n**Token Consumption**\\n\\nToken consumption refers to the number of tokens used by a generative AI model to generate a response. Tokens are units of measurement that represent the amount of text that a model can generate. The number of tokens consumed by a model depends on a number of factors, including the length of the input text, the complexity of the task, and the model's architecture.\\n\\n**How to calculate token consumption**\\n\\nTo calculate token consumption, you can use the following formula:\\n\\n```\\nToken consumption = Number of words in the output text / Average word length\\n```\\n\\nFor example, if the output text is 100 words long and the average word length is 5 characters, then the token consumption would be 20.\\n\\n**How to optimize token consumption**\\n\\nThere are a number of ways to optimize token consumption, including:\\n\\n* Using shorter input text\\n* Simplifying the task\\n* Using a more efficient model\\n\\n**Why is token consumption important?**\\n\\nToken consumption is important because it can affect the cost of using a generative AI model. Models that consume more tokens are typically more expensive to use.\\n\\n**Additional information**\\n\\nHere are some additional things to keep in mind about token consumption:\\n\\n* Token consumption is not always a good indicator of the quality of a model's output.\\n* Some models are more efficient than others at generating text.\\n* You can use a token calculator to estimate the token consumption of a given model.\\n\\nI hope this helps! Let me know if you have any other questions.\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["quest = \"what do developers do to reduce the number of LLM requests?\"\n","ask_question_with_parameters(quest)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":420},"id":"dSX4k1YgrbJL","executionInfo":{"status":"ok","timestamp":1720991456131,"user_tz":420,"elapsed":113445,"user":{"displayName":"Nick Lee","userId":"01521670819627041880"}},"outputId":"6e5bd8e2-a11f-4a3f-f5cf-2b891f3e63be"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n","  warnings.warn(Warnings.W108)\n","WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name bert-base-uncased. Creating a new one with mean pooling.\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 152 is greater than number of elements in index 8, updating n_results = 8\n","WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n","WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n","WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 152 is greater than number of elements in index 8, updating n_results = 8\n","WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 20 is greater than number of elements in index 8, updating n_results = 8\n","WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 20 is greater than number of elements in index 8, updating n_results = 8\n","WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 20 is greater than number of elements in index 8, updating n_results = 8\n","WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 20 is greater than number of elements in index 8, updating n_results = 8\n","WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 20 is greater than number of elements in index 8, updating n_results = 8\n","WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 152 is greater than number of elements in index 8, updating n_results = 8\n","WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n","WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n","WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 8.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n","WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 16.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n","WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 32.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"]},{"output_type":"execute_result","data":{"text/plain":["'Developers can batch requests or implement delays between subsequent requests to reduce the number of LLM requests. They can also track the number of requests sent or identify this from the error message/API endpoint to determine the degree to which they need to limit the requests.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["quest = \"what is Parent-Child Document Retriever?\"\n","ask_question_with_parameters(quest)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":455},"id":"0wq9PaWyrbNV","executionInfo":{"status":"ok","timestamp":1720991643917,"user_tz":420,"elapsed":105398,"user":{"displayName":"Nick Lee","userId":"01521670819627041880"}},"outputId":"6af46e5f-a1b8-430e-c560-580ea8c43590"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n","  warnings.warn(Warnings.W108)\n","WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name bert-base-uncased. Creating a new one with mean pooling.\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 152 is greater than number of elements in index 10, updating n_results = 10\n","WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 152 is greater than number of elements in index 10, updating n_results = 10\n","WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 20 is greater than number of elements in index 10, updating n_results = 10\n","WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 20 is greater than number of elements in index 10, updating n_results = 10\n","WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 20 is greater than number of elements in index 10, updating n_results = 10\n","WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 20 is greater than number of elements in index 10, updating n_results = 10\n","WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 20 is greater than number of elements in index 10, updating n_results = 10\n","WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 152 is greater than number of elements in index 10, updating n_results = 10\n","WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n","WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n","WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 8.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n","WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 16.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n","WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 32.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"]},{"output_type":"execute_result","data":{"text/plain":["'The Parent-Child Document Retriever is a multi-level chunking strategy used in RAG applications. It first performs vector similarity search, retrieves the chunks associated with the most similar vectors, and includes these chunks in the LLM prompt. Vector embeddings account for semantics at a local level (adjacent words) and at a global level (entire paragraphs/pages). For this reason, vector embeddings for large chunks may include extra noise if the local semantic meaning differs from the global semantic meaning, which is likely the case. In general, similarity search performance is lower for large embeddings, which correspond to large chunk sizes. This behavior makes it challenging to determine the optimal chunk size, since similarity search performs better with smaller chunks, but the LLM prompt needs adequate context, so it requires larger chunks.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["quest = \"Why do the RAG applications benefit from using a Parent-Child Document Retriever?\"\n","ask_question_with_parameters(quest)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":507},"id":"c8vhO9PtrbTE","executionInfo":{"status":"ok","timestamp":1720991764296,"user_tz":420,"elapsed":113584,"user":{"displayName":"Nick Lee","userId":"01521670819627041880"}},"outputId":"45fa2c07-f0d5-43a3-ed7b-f7a7d117e667"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n","  warnings.warn(Warnings.W108)\n","WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name bert-base-uncased. Creating a new one with mean pooling.\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 152 is greater than number of elements in index 12, updating n_results = 12\n","WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n","WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n","WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 152 is greater than number of elements in index 12, updating n_results = 12\n","WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 20 is greater than number of elements in index 12, updating n_results = 12\n","WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 20 is greater than number of elements in index 12, updating n_results = 12\n","WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 20 is greater than number of elements in index 12, updating n_results = 12\n","WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 20 is greater than number of elements in index 12, updating n_results = 12\n","WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 20 is greater than number of elements in index 12, updating n_results = 12\n","WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 152 is greater than number of elements in index 12, updating n_results = 12\n","WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n","WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n","WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 8.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n","WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 16.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n","WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 32.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"]},{"output_type":"execute_result","data":{"text/plain":["'RAG applications benefit from using a Parent-Child Document Retriever because it utilizes a multi-level chunking strategy. RAG applications first perform vector similarity search, retrieve the chunks associated with the most similar vectors, and include these chunks in the LLM prompt. Vector embeddings account for semantics at a local level (adjacent words) and at a global level (entire paragraphs/pages). For this reason, vector embeddings for large chunks may include extra noise if the local semantic meaning differs from the global semantic meaning, which is likely the case. In general, similarity search performance is lower for large embeddings, which correspond to large chunk sizes. This behavior makes it challenging to determine the optimal chunk size, since similarity search performs better with smaller chunks, but the LLM prompt needs adequate context, so it requires larger chunks.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":16}]}]}